{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import import_dataset as load\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "\n",
    "\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debut chargement image\n",
      "debut chargement image\n",
      "(12000, 3, 224, 224)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "#Importation du data\n",
    "train_path = \"../dataset/train\" \n",
    "test_path = \"../dataset/test\" \n",
    "\n",
    "train_dataset_X, train_dataset_Y = load.loadDataset(train_path)\n",
    "test_dataset_X, test_dataset_Y = load.loadDataset(test_path)\n",
    "\n",
    "print(train_dataset_X.shape)\n",
    "print(train_dataset_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8580, 3, 224, 224)\n",
      "(8580,)\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset_X.shape)\n",
    "print(test_dataset_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Y labels\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_dataset_Y = encoder.fit_transform(train_dataset_Y)\n",
    "test_dataset_Y = encoder.fit_transform(test_dataset_Y)\n",
    "\n",
    "print(np.unique(test_dataset_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12000, 3, 224, 224])\n",
      "torch.Size([12000])\n",
      "torch.Size([8580, 3, 224, 224])\n",
      "torch.Size([8580])\n"
     ]
    }
   ],
   "source": [
    "#Create custom dataLoader\n",
    "\n",
    "class DogsDataset(Dataset):\n",
    "    \"\"\" Dogs dataset.\"\"\"\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self, X, Y):\n",
    "        self.len = Y.shape[0]\n",
    "        self.x_data = torch.from_numpy(X).float()\n",
    "        self.y_data = torch.from_numpy(Y).float()\n",
    "        print(self.x_data.shape)\n",
    "        print(self.y_data.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset_train = DogsDataset(train_dataset_X, train_dataset_Y)\n",
    "dataset_test = DogsDataset(test_dataset_X, test_dataset_Y)\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset_train,\n",
    "                          batch_size=100,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(3, 10, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(10),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(10, 20, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(20),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(62720, 120) # A  changer soon \n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=62720, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# model.to(device)\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "# /\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(inputs)\n",
    "\n",
    "        print(\"output_train\")\n",
    "        print(output_train)\n",
    "        print(\"labels\")\n",
    "        print(labels)\n",
    "        loss_train = criterion(output_train, labels.long())\n",
    "        train_losses.append(loss_train)\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss_train.item()\n",
    "        # i1 = i1 + 100\n",
    "        # i2 = i2 + 100\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_train)\n",
    "    return tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_train\n",
      "tensor([[ -455.4199,   -45.0914,   435.4480,  ...,  -579.5044,  -396.9266,\n",
      "          -220.9962],\n",
      "        [ -839.3218,   377.1429,   496.6088,  ..., -1009.4958,  -251.4881,\n",
      "          -666.9822],\n",
      "        [ -726.3638,  -489.0055,  1059.3944,  ..., -2564.5190,  -626.3351,\n",
      "          -277.0496],\n",
      "        ...,\n",
      "        [ -306.7890,   297.2759,   202.0193,  ...,  -188.3092,  -388.0200,\n",
      "          -280.3269],\n",
      "        [ -552.5507,   319.7392,   414.8871,  ...,  -764.8431,  -261.1429,\n",
      "          -351.9996],\n",
      "        [ -445.9676,   450.6695,   333.8914,  ...,  -527.0980,  -511.5997,\n",
      "          -377.7416]], grad_fn=<AddmmBackward>)\n",
      "labels\n",
      "tensor([ 55.,  59.,  36.,  18.,  87.,   7.,  27.,   3., 107.,  75.,  99.,  54.,\n",
      "         16.,  86.,   9.,  64., 102.,  95.,   7., 117.,  74.,  27.,   8.,   8.,\n",
      "         95.,  10.,  72.,  42.,  71.,  56.,  10.,  50.,  33.,  56.,   3.,  26.,\n",
      "         69.,  51., 110., 100.,  54., 104.,  83.,  43.,  35.,  26.,  59.,  22.,\n",
      "         94.,  98.,   1.,  93.,  90.,  23.,  29.,  72., 103.,  45.,  74.,   8.,\n",
      "         55., 117.,  65.,  66.,  44.,  32.,  22.,  56.,  53.,  89.,  18.,  51.,\n",
      "         74.,  64., 110.,  58.,  89.,  95., 104.,  39.,   0.,  48.,  40.,  16.,\n",
      "         51.,  53., 106.,  29.,  90.,  53., 110.,  78.,  52.,  84.,  78.,  48.,\n",
      "         41.,  23.,  96., 109.])\n",
      "output_train\n",
      "tensor([[-1313.4532,  -180.6274,   745.5970,  ..., -3187.0640,   773.3409,\n",
      "          -135.5716],\n",
      "        [ -298.2750,   298.3198,   141.4499,  ...,  -610.5791,   -12.3280,\n",
      "          -100.5098],\n",
      "        [ -280.5333,   288.0205,   165.0496,  ...,  -313.1298,   -65.3979,\n",
      "          -205.6526],\n",
      "        ...,\n",
      "        [-1364.2500,  -155.2286,   840.1868,  ..., -3355.0376,   776.5168,\n",
      "          -428.8097],\n",
      "        [  -27.1181,   -39.5562,   131.6439,  ...,  -257.0082,  -193.3863,\n",
      "            43.5757],\n",
      "        [ -867.4687,    13.3949,   539.2134,  ..., -1918.5254,   425.0730,\n",
      "          -177.8981]], grad_fn=<AddmmBackward>)\n",
      "labels\n",
      "tensor([ 98., 100.,  65.,  29.,  59.,  75.,  96.,  76.,  13.,   3.,  87.,  63.,\n",
      "         58., 104.,  87.,   9.,  36.,  97.,  63.,  98.,  33., 107.,  24.,  46.,\n",
      "         86., 112.,  96.,  48., 108., 107.,  49.,   1.,  98.,  32.,   3., 115.,\n",
      "         77.,  87.,  94., 116.,  23.,  75.,  71.,  39.,  16.,  28., 105., 104.,\n",
      "          0., 105.,  90.,  62.,   1.,  64.,  40., 112.,  92.,  62.,  66.,  73.,\n",
      "        119.,  89.,  98.,  96.,  85.,   3.,  85.,  75.,  34.,  85.,  88.,  65.,\n",
      "         52.,   6.,  92.,  46.,  20., 114.,  37., 102.,  64., 104.,  49.,  46.,\n",
      "         51.,  14.,   1.,  63.,  18.,  37.,  29.,  93., 104.,  64., 104.,  52.,\n",
      "        100.,  29.,  69., 109.])\n",
      "output_train\n",
      "tensor([[  -11.3597,   -54.8106,    83.4224,  ...,  -678.9799,   180.4776,\n",
      "           260.9637],\n",
      "        [  -25.5331,    38.1588,     6.8309,  ...,   -35.5790,    -8.0922,\n",
      "           -20.9206],\n",
      "        [   -8.3037,    16.9632,     5.4864,  ...,    -7.5288,    -1.9208,\n",
      "           -10.3519],\n",
      "        ...,\n",
      "        [  -39.8920,   -71.8936,   124.1039,  ..., -1093.2559,   356.1269,\n",
      "           262.5696],\n",
      "        [ -135.4780,   154.8861,    84.4288,  ...,  -240.6047,   -68.2966,\n",
      "            36.3178],\n",
      "        [    1.4379,     2.5044,     8.4369,  ...,   -43.7493,    -6.7337,\n",
      "            18.3402]], grad_fn=<AddmmBackward>)\n",
      "labels\n",
      "tensor([ 66.,  13.,  78.,  76.,   3.,  43.,  97.,  66.,  18.,   5.,   0.,  54.,\n",
      "          8.,  91., 119.,  83.,  11.,  32.,  21.,  18.,   4.,   8.,  51.,  33.,\n",
      "        101.,  27.,  30.,  82.,  39., 102.,  88.,  16.,  98.,  48., 104.,  88.,\n",
      "        103.,  24.,  52.,   6.,  96.,   0.,  64.,  43.,  23.,  30.,  28.,  31.,\n",
      "         46.,  98.,  32.,  42., 116.,  67.,  93.,  56.,  53.,  89.,  70.,  71.,\n",
      "         72.,  63.,  25.,  72.,  54.,  60.,  59., 106.,  31.,  82.,  13.,  73.,\n",
      "        101.,  46.,  64.,  18.,  21., 117.,  78.,  15.,  68.,   5.,  29.,  50.,\n",
      "        111.,  55.,  94.,  47., 118.,  70.,  16.,  10., 107.,   0.,  31.,  87.,\n",
      "        118.,   6., 101.,  80.])\n",
      "output_train\n",
      "tensor([[ 7.3946e+00,  3.1745e+01,  6.9685e+00,  ..., -7.1576e+01,\n",
      "          6.6789e+00,  2.4991e+01],\n",
      "        [-6.8973e+00,  2.1336e+01,  4.3808e-01,  ..., -2.1574e+01,\n",
      "         -2.5950e-01,  4.5487e-02],\n",
      "        [ 1.4482e+01,  3.7165e+00,  3.2109e+00,  ..., -6.5534e+01,\n",
      "          1.1471e+01,  6.6785e+00],\n",
      "        ...,\n",
      "        [ 5.0181e+01, -2.7723e+01, -3.0576e+00,  ..., -5.9050e+01,\n",
      "         -6.9529e+01,  8.5887e+01],\n",
      "        [ 9.3633e+00,  2.7931e+01,  6.7017e+00,  ..., -1.0021e+02,\n",
      "          1.0654e+01,  3.8349e+01],\n",
      "        [ 3.9145e+00,  1.6456e+00,  4.3134e+00,  ..., -4.0675e+01,\n",
      "          1.6863e+01,  6.5632e+00]], grad_fn=<AddmmBackward>)\n",
      "labels\n",
      "tensor([  8.,  70.,  88.,  22.,  25.,  77., 119.,  90.,  21.,  77.,  28.,  46.,\n",
      "         57.,  56.,  86.,  62.,  69.,  46.,  67., 104.,  68.,   0.,  82.,  72.,\n",
      "          5.,  95.,  67., 116.,  47.,  41., 104.,  90., 119.,  54.,  76.,  61.,\n",
      "         92.,  33.,  14.,  91.,  18.,  27.,  36.,  25.,  26.,  71.,  85.,  92.,\n",
      "         54.,  98.,  48.,  90.,  68.,  10.,  57.,  11.,  39.,  50.,  74.,  22.,\n",
      "         43.,  54.,  67.,  31.,  62.,  43.,  85.,  61.,  85.,  92.,  74., 100.,\n",
      "         34.,  67.,  71.,  98.,  33.,  80.,  86., 103.,  28., 114.,  66.,  63.,\n",
      "        103.,  91.,  67.,  86.,  94.,   3.,  50.,  91.,  51.,  73., 104.,  97.,\n",
      "         23.,  43.,  65.,  22.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ae9cd3fcd056>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-7f6982162fa4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# defining the number of epochs\n",
    "n_epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and predictions : \n",
    "\n",
    "test_dataset_X = test_dataset_X.reshape(8580, 1, 224, 224)\n",
    "test_dataset_X  = torch.from_numpy(test_dataset_X).float()\n",
    "test_dataset_Y = torch.from_numpy(test_dataset_Y).float()\n",
    "\n",
    "print(test_dataset_X.shape)\n",
    "print(test_dataset_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('./model.pth'))\n",
    "# generating predictions for test set\n",
    "# output = model(test_dataset_X)\n",
    "\n",
    "# softmax = torch.nn.Softmax(output)\n",
    "# prob = list(softmax.numpy())\n",
    "# predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# # accuracy on training set\n",
    "# accuracy_score(train_y, predictions)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "        outputs = model(test_dataset_X)\n",
    "        # predicted = torch.nn.Softmax(outputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += test_dataset_Y.size(0)\n",
    "        correct += (predicted == test_dataset_Y).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
