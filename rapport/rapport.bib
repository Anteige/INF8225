@inproceedings{KhoslaYaoJayadevaprakashFeiFei_FGVC2011,
author = "Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and Li Fei-Fei",
title = "Novel Dataset for Fine-Grained Image Categorization",
booktitle = "First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition",
year = "2011",
month = "June",
address = "Colorado Springs, CO",
}

@inproceedings{43022,
title = {Going Deeper with Convolutions},
author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year = {2015},
URL = {http://arxiv.org/abs/1409.4842},
booktitle = {Computer Vision and Pattern Recognition (CVPR)}
}

@misc{tsang_2018, title={Medium}, url={https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7}, journal={Medium}, author={Tsang, Sik Ho}, year={2018}, month={Aug}} 

@misc{lin2013network,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN)
to enhance model discriminability for local patches within the receptive field.
The conventional convolutional layer uses linear filters followed by a
nonlinear activation function to scan the input. Instead, we build micro neural
networks with more complex structures to abstract the data within the receptive
field. We instantiate the micro neural network with a multilayer perceptron,
which is a potent function approximator. The feature maps are obtained by
sliding the micro networks over the input in a similar manner as CNN; they are
then fed into the next layer. Deep NIN can be implemented by stacking mutiple
of the above described structure. With enhanced local modeling via the micro
network, we are able to utilize global average pooling over feature maps in the
classification layer, which is easier to interpret and less prone to
overfitting than traditional fully connected layers. We demonstrated the
state-of-the-art classification performances with NIN on CIFAR-10 and
CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  added-at = {2020-03-01T10:35:05.000+0100},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  biburl = {https://www.bibsonomy.org/bibtex/2fde7e76ced46f6474efb0bbe0dc335a6/stdiff},
  description = {Network In Network},
  interhash = {67da1ef3d0d91a16dbe3ec25474b9aaf},
  intrahash = {fde7e76ced46f6474efb0bbe0dc335a6},
  keywords = {neural-network},
  note = {cite arxiv:1312.4400Comment: 10 pages, 4 figures, for iclr2014},
  timestamp = {2020-03-01T10:35:05.000+0100},
  title = {Network In Network},
  url = {http://arxiv.org/abs/1312.4400},
  year = 2013
}

@techreport{fcdh_FinalReport,
author="David Hsu",
title="Using Convolutional Neural Networks to Classify Dog Breeds",
institution="Standford",
year="2015"
}

@techreport{output,
author="Whitney LaRow and Brian Mittl and Vijay Singh",
title="Dog Breed Identification",
institution="Standford",
year="2016"
}
